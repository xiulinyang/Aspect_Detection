# -*- coding: utf-8 -*-
"""ner_template.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lQm6bYSECvpedNX53sltvGybnXs_O5yw
"""

! pip install nltk==3.7

! pip install transformers

import pandas as pd
from nltk import ngrams
import random
from random import sample
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments
from sklearn.utils import shuffle

from google.colab import drive
drive.mount('/content/gdrive')

def get_template(asp, sents, n_gram):
    ngram_sents =[]
    for sent in sents:
        single_sent =[]
        for i in range(1,n_gram+1):
            single_sent.append(ngrams(sent.split(), i))
        ngram_sents.append(single_sent)
    non_aspect_template =[]
    aspect_template= []
    for i, sent in enumerate(ngram_sents):
      for n_gram in sent:
        for n in n_gram:
          text =''
          n = ' '.join(list(n))
          if n in asp[i]:
            text+=f'{sents[i].lower()} is {n} an aspect: '
            aspect_template.append(text)

          else:
            text += f'{sents[i].lower()} is {n} an aspect: '
            non_aspect_template.append(text)
 
    return aspect_template, non_aspect_template

class NerDataset:
  def __init__(self, text_list, asp_list, tokenizer, max_length):
    self.input_ids =[]
    self.attn_masks =[]
    asp_tmp, non_asp_temp = get_template(asp_list, text_list, n_gram=7)
    text_list = asp_tmp+non_asp_temp
    non_asp_temp = sample(non_asp_temp, int(len(asp_tmp)*1.5))
    self.labels = ['yes']*len(asp_tmp) +['no']*len(non_asp_temp)
    for sent, label in zip(text_list, self.labels):
      prep_text = f'<startoftext> {sent.lower()} {label} <endoftext>'
      encodings_dict = tokenizer(prep_text, truncation=True,
                                   max_length=max_length, padding ='max_length')
      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))
      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))
  def __len__(self):
    return len(self.input_ids)

  def __getitem__(self, idx):
    return self.input_ids[idx], self.attn_masks[idx], self.labels[idx]

def get_data(dir, tokenizer):
    data = pd.read_json(dir, lines=True)
    asp = data['aspect_pos_string'].to_list()
    sents = data['sentence'].to_list()
    ner_data = NerDataset(sents, asp, tokenizer, max_length=512)

    return ner_data

from transformers import AutoConfig, AutoModel

config = AutoConfig.from_pretrained("/content/gdrive/MyDrive/checkpoint-19368")
model = GPT2LMHeadModel(config=config)

torch.manual_seed(123)
tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<startoftext', 
                                          eos_token='<endoftext>', pad_token='<pad>')

# model= GPT2LMHeadModel.from_pretrained(model_name).cuda()
model.resize_token_embeddings(len(tokenizer))

dev_dir = '/content/gdrive/MyDrive/in_topic/dev.jsonl'
train_dir ='/content/gdrive/MyDrive/in_topic/train.jsonl'
test_dir ='/content/gdrive/MyDrive/in_topic/test.jsonl'

train_dataset = get_data(train_dir, tokenizer)
X_dev, y_dev = get_data(dev_dir, tokenizer)

training_args= TrainingArguments(output_dir='/content/gdrive/MyDrive/in_topic/results', num_train_epochs=2, logging_steps=100,
                                  save_strategy='epoch', 
                                 per_device_train_batch_size=2, per_device_eval_batch_size=2,
                                 warmup_steps=100, weight_decay=0.01, logging_dir='/content/gdrive/MyDrive/in_topic/logs')
Trainer(model=model, args=training_args, train_dataset=(train_dataset), eval_dataset=dev_dataset, 
        data_collator=lambda data:{'input_ids': torch.stack([f[0] for f in data]),
                                   'attention_mask': torch.stack([f[1] for f in data]),
                                   'labels': torch.stack([f[0] for f in data])}).train()

import re
import tqdm

_ = model.eval()
model.to('cuda')
test_dir ='/content/gdrive/MyDrive/in_topic/test.jsonl'
test_data = pd.read_json(test_dir, lines=True)
test_sent = test_data['sentence'].to_list()
test_aspect = test_data['aspect_pos_string'].to_list()
asp_tem_test, non_asp_test = get_template(test_aspect, test_sent, 7)
random.seed = 123
non_asp_test = sample(non_asp_test, int(len(asp_tem_test)*1.5))
y_test = ['yes']*len(asp_tem_test)+['no']*len(non_asp_test)

X_test = asp_tem_test + non_asp_test

X_test, y_test = shuffle(X_test, y_test, random_state=123)
predicted_list =[]
count =0
for text, label in zip(X_test, y_test):
  count+=1
  print(count)
  generated= tokenizer(f'{text}', return_tensors='pt').input_ids.cuda()
  sample_outputs = model.generate(generated, do_sampe=False, top_k=1, max_length=512, top_p=0.90,
                                  temperature =0.7, num_return_sequences=1)
  predicted_text = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)
  aspect_answer = re.findall(r'an aspect:(.*)', predicted_text)
  asp_answer = aspect_answer[0].strip()

  predicted_list.append(asp_answer)

print(len(X_test))

from sklearn.metrics import classification_report

print(classification_report(y_test, predicted_list, target_names=['yes', 'no']))